# -*- coding: utf-8 -*-
"""HR_Attrition final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Be3UjXU5FpbjaCUGPd0N2_lftuPZ1beb

## STEP 1

# Downloading the dataset using the opendatasets Python library
"""

!pip install opendatasets
import opendatasets as od

data_path = "https://www.kaggle.com/singhnproud77/hr-attrition-dataset?select=Final+dataset+Attrition.csv"
od.download(data_path)

"""## STEP 2

# Importing all necessary libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline


from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings('ignore')

"""# Pandas Dataframes: DataFrame is an object for data manipulation. It is a 2D tabular structure, where every row is a dataset entry and columns represent features of data."""

df =  pd.read_csv("Final dataset Attrition.csv")

"""# We can have a look to first few rows of the table using head function"""

# reading first five datapoints
df.head()

df.head(10)

#all the columns name
df.columns

"""# Shape Function"""

#check for the no. of columns & rows
df.shape

"""## Data Types"""

# Checking the data types of all the columns
df.dtypes

"""# Describe function"""

df.describe()

"""# isnull Function"""

df.isnull()

"""# Missing Data Values"""

# Checking for null values
df.isnull().sum()

"""There are two rows having null values & they are having all null values, so we can drop those columns.

## Droping the two columns
"""

df = df.drop(["Date_of_termination","Unnamed: 32"], axis=1)

# new df after removing two columns
df

## Below are some codes to know about how many categories are there
## for each categorical variable & number of the categories they have

df["Attrition"].value_counts()

df["BusinessTravel"].value_counts()

df["Department"].value_counts()

df["Gender"].value_counts()

df["JobRole"].value_counts()

df["Higher_Education"].value_counts()

df["Status_of_leaving"].value_counts()

df["Mode_of_work"].value_counts()

pd.to_datetime(df["Date_of_Hire"])

"""## Parsing Date in Month and Year"""

## Parsing Date in Month & year
df['day'] = pd.DatetimeIndex(df['Date_of_Hire']).day
df['year'] = pd.DatetimeIndex(df['Date_of_Hire']).year
df['month'] = pd.DatetimeIndex(df['Date_of_Hire']).month

df

"""## Outliers"""

# Outlier detection
df1=df.select_dtypes(exclude=['object'])
for column in df1:
    plt.figure()
    df1.boxplot([column])

"""## STEP 3

### Basic Statistics
"""

df.corr()

# description of all numerical items
# min,max,mean,count are important data displayed using this command
df.describe()

"""# Mean"""

print(df.mean())

"""# Median"""

print(df.median())

"""# Mode"""

from scipy import stats
print(df.mode())

"""# Variance"""

import statistics
df.var()

"""##  Standard Deviation"""

print(df.std()) # numpy

"""## Normal Distribution


"""

from numpy.linalg import norm
print(norm(df['Age']))

"""##  Uniform Distribution"""

from scipy.stats import uniform
print(np.random.uniform(df['Age']))

"""## Histograms"""

df.hist(alpha=.8, figsize=(10,8))

"""###  Univariate Analysis"""

p = sns.countplot(x=df['Attrition'], data=df)

"""From abvove plot we can see that the data is imbalanced wrt the target variable"""

p = sns.countplot(x=df['Department'], data=df)

"""Human resources are very less in number"""

p = sns.countplot(x=df['Gender'], data=df)

p = sns.countplot(x=df['Source_of_Hire'], data=df)

p = sns.countplot(x=df['Job_mode'], data=df)

"""###  Bivariate Analysis"""

sns.lineplot(data=df, x="PercentSalaryHike", y="PerformanceRating")

"""## Scatter Plot"""

sns.scatterplot(data=df, y="JobSatisfaction", x="Attrition")

sns.scatterplot(data=df, y="NumCompaniesWorked", x="Attrition")

sns.scatterplot(data=df, x="JobLevel", y="Age")

sns.scatterplot(data=df, y="MonthlyIncome", x="StockOptionLevel")

sns.scatterplot(data=df, y="MonthlyIncome", x="Attrition")

sns.scatterplot(data=df, y="YearsAtCompany", x="Attrition")

"""### Trivariate Analysis"""

sns.barplot(y="MonthlyIncome",
           x="Job_mode",
           hue="Gender",
           data= df)

sns.barplot(y="MonthlyIncome",
           x="Attrition",
           hue="Gender",
           data= df)

sns.barplot(y="MonthlyIncome",
           x="Attrition",
           hue="Job_mode",
           data= df)

"""## Pandas Profling ( HTML output)"""

!pip install pandas-profiling

from pandas_profiling import ProfileReport
prof = ProfileReport(df)
prof.to_file(output_file='output.html')

"""## STEP 4

### Questions

## Q1 - Is the dataset imbalanced or balanced wrt the target variable?

## Q2 - Is the dataset good to implement an ML algorithm?

## Q3 - Were there any missing values in your dataset? How did you handled them & what was the reason behind choosing that technique?

## Q4 - Are the categorical features like gender, Job_mode creating affecting attrition(target variable)?

## STEP 5

## Inferences & conclusion
Machine learning (ML) allows us to make our computers learn to make predictions and decisions based on data and learn from experiences.
    -We can easily find patterns & trends by pushing our data to the ML algorithms. With ML, you donâ€™t need to babysit your          project every step of the way. Since it means giving machines the ability to learn, it lets them make predictions and also      improve the algorithms on their own.
    -We can see various statistical facts about our data just by giving simple python commands
    -We can visualize all the features that we have in our dataset, we can perform various kind of plots that could                  benificial for us to understand more about our data

## Model Building
"""

# Importing Models
!pip install xgboost
!pip install lightgbm

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier

"""### Encoding categorical Columns"""

## We need to change categorical columns in numerical column before we fit any model

# Getting names of all the categorical columns
df.select_dtypes(include=['object']).columns.tolist()

## Encoding the categorical features
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Attrition'] = le.fit_transform(df['Attrition'])
encoded_df = pd.get_dummies(df, columns = [
 'BusinessTravel',
 'Department',
 'Gender',
 'JobRole',
 'MaritalStatus',
 'OverTime',
 'Higher_Education',
 'Date_of_Hire',
 'Status_of_leaving',
 'Mode_of_work',
 'Work_accident',
 'Source_of_Hire',
 'Job_mode'])

# Dataframe after encoding all the categorical variables
encoded_df

# Splitting the features from target feature
y = encoded_df["Attrition"]
X = encoded_df.drop(["Attrition"], axis=1)

# performing startified sampling due to imbalanced dataset
# another option could have been the upsampling technique
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y,
                                                    test_size=0.2)

#checking test train shape
names =[X_train,X_test,y_train,y_test]
for name in names:
    print(name.shape)

# XGB Classifier
model = XGBClassifier()
# fit the model with the training data
model.fit(X_train,y_train)

# predict the target on the train dataset
predict_train = model.predict(X_train)

# Accuray Score on train dataset
accuracy_train = accuracy_score(y_train,predict_train)
print("Accuracy on train data-", accuracy_train)

# predict the target on the test dataset
predict_test = model.predict(X_test)
# Accuray Score on test dataset
accuracy_test = accuracy_score(y_test,predict_test)
print("Accuracy on test data-", accuracy_test)

# LGBM Classifier
model = LGBMClassifier()
# fit the model with the training data
model.fit(X_train,y_train)

# predict the target on the train dataset
predict_train = model.predict(X_train)

# Accuray Score on train dataset
accuracy_train = accuracy_score(y_train,predict_train)
print("Accuracy on train data-", accuracy_train)

# predict the target on the test dataset
predict_test = model.predict(X_test)

# Accuray Score on test dataset
accuracy_test = accuracy_score(y_test,predict_test)
print("Accuracy on test data-", accuracy_test)

# RF Classifier
model = RandomForestClassifier()
# fit the model with the training data
model.fit(X_train,y_train)

# predict the target on the train dataset
predict_train = model.predict(X_train)

# Accuray Score on train dataset
accuracy_train = accuracy_score(y_train,predict_train)
print("Accuracy on train data-", accuracy_train)

# predict the target on the test dataset
predict_test = model.predict(X_test)

# Accuray Score on test dataset
accuracy_test = accuracy_score(y_test,predict_test)
print("Accuracy on test data-", accuracy_test)

# Adaboost Classifier
model = AdaBoostClassifier()
# fit the model with the training data
model.fit(X_train,y_train)

# predict the target on the train dataset
predict_train = model.predict(X_train)

# Accuray Score on train dataset
accuracy_train = accuracy_score(y_train,predict_train)
print("Accuracy on train data-", accuracy_train)

# predict the target on the test dataset
predict_test = model.predict(X_test)

# Accuray Score on test dataset
accuracy_test = accuracy_score(y_test,predict_test)
print("Accuracy on test data-", accuracy_test)

predict_test

y_test

## For fine tuning our model we can perform Hyperparameter tuning on the model which is giving best accuracy eg RandomForest

"""## Saving/Loading our model for future use"""

import pickle
filename = 'model.sav'
pickle.dump(model, open(filename, 'wb'))

load_model = pickle.load(open(filename, 'rb'))

predict_test = load_model.predict(X_test)

predict_test